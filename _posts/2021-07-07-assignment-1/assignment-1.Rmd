---
title: "Visual Analytics Assignment - Vast Challenge 2021"
description: |
  Apply concepts, methods and techniques learnt in class to solve real world problem using R.
  This was created as part of the course requirement for ISSS608 Visual Analytics for MITB.
author:
  - name: Aaron Oh
    url: https://www.linkedin.com/in/aaronoh743/
    affiliation: School of Computing and Information Systems, Singapore Management University
    affiliation_url: https://scis.smu.edu.sg/
    date: 07-17-2021

output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# 1. Introduction


For the past 20 years, Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos. They have been largely successful in generating profits and developed strong relationships with the government of Kronos. Unfortunately, their business has not been successful in demonstrating environmental stewardship with the Protectors of Kronos (POK). In January 2014, several employees of GAStech went missing in the midst of a celebration. POK is suspected in the disappearances. 

In this mini-challenge, we are tasked to investigate the relationships and conditions that led up to the kidnapping. I will be answering the following questions:

1. Characterize the news data sources provided. Which are primary sources and which are derivative sources? What are the relationships between the primary and derivative sources?

2. Characterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. 

3. Given the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. Include both personal relationships and shared goals and objectives. Provide evidence for these relationships.


# 2. Literature Review

By definition, primary sources are first-hand account of an event and typically represent an original view, reporting on discoveries and events, or sharing new information. A derivative source, on the other hand, typically involves an analysis and/or interpretation of the primary sources. Given the definitions, derivative sources have 2 main traits - (1) similar content to the primary source(s) and (2)published after the primary sources.Using these traits, it is likely to create a network between the primary and secondary sources, and subsequently deep-dive to understand what are the articles discussing. 





# 3. Question 1 Analysis

Before we can begin analysing the questions, I will first set up the packages required for this project. It can be done with the code chunk listed below.

```{r, echo = TRUE}

packages = c('tidytext','widyr','wordcloud',
             'DT','ggwordcloud','textplot',
             'lubridate', 'hms', 'tidyverse',
             'tidygraph', 'ggraph', 'igraph','stringr','tidyr', 'ggplot2',
             'visNetwork', 'topicmodels', 'crosstalk')

for (p in packages) {
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

```

## 2.1 Data Preparation

First, I have loaded the news articles required for the first question. The code below will read individual text file as one id group and save it as a dataframe

```{r, echo = TRUE}

list_of_files <- "C:/Users/aaron/Documents/Aaron Documents/Semester 3/VA/Assignment/Raw Data/News Articles"

read_folder <- function (infolder) {
  tibble(file = dir(infolder,
                    full.names = TRUE)) %>%
    mutate(text = map(file,
                      read_lines)) %>%
    transmute(id=basename(file),
              text) %>%
    unnest(text)
}



raw_text <- tibble(folder =
                     dir(list_of_files,
                         full.names=TRUE)) %>%
  mutate(folder_out = map(folder,
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder),
            id, text)
write_rds(raw_text, "C:/Users/aaron/Documents/Aaron Documents/Semester 3/VA/Assignment/Raw Data/News Articles/allfiles.rds")

for (col in colnames(raw_text)){
  Encoding(raw_text[[col]]) <- "latin1"}

```

Next, I made use of a ggplot to ensure that the number of files uploaded matches the raw data. I have done a visual check with the files in my repository.

```{r, echo = FALSE}
raw_text %>% 
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y=NULL)
```

