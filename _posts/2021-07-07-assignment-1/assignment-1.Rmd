---
title: "Visual Analytics Assignment - Vast Challenge 2021"
description: |
  Apply concepts, methods and techniques learnt in class to solve real world problem using R.
  This was created as part of the course requirement for ISSS608 Visual Analytics for MITB.
author:
  - name: Aaron Oh
    url: https://www.linkedin.com/in/aaronoh743/
    affiliation: School of Computing and Information Systems, Singapore Management University
    affiliation_url: https://scis.smu.edu.sg/
    date: 07-17-2021

output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# 1. Introduction


For the past 20 years, Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos. They have been largely successful in generating profits and developed strong relationships with the government of Kronos. Unfortunately, their business has not been successful in demonstrating environmental stewardship with the Protectors of Kronos (POK). In January 2014, several employees of GAStech went missing in the midst of a celebration. POK is suspected in the disappearances. 

In this mini-challenge, we are tasked to investigate the relationships and conditions that led up to the kidnapping. I will be answering the following questions:

1. Characterize the news data sources provided. Which are primary sources and which are derivative sources? What are the relationships between the primary and derivative sources?

2. Characterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. 

3. Given the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. Include both personal relationships and shared goals and objectives. Provide evidence for these relationships.


# 2. Literature Review

By definition, primary sources are first-hand account of an event and typically represent an original view, reporting on discoveries and events, or sharing new information. A derivative source, on the other hand, typically involves an analysis and/or interpretation of the primary sources. Given the definitions, derivative sources have 2 main traits - (1) similar content to the primary source(s) and (2)published after the primary sources.Using these traits, it is likely to create a network between the primary and secondary sources, and subsequently deep-dive to understand what are the articles discussing. 


# 3. Question 1 Analysis

Before we can begin analysing the questions, I will first set up the packages required for this project. It can be done with the code chunk listed below.

```{r, echo = TRUE}

packages = c('tidytext','widyr','wordcloud',
             'DT','ggwordcloud','textplot',
             'lubridate', 'hms', 'tidyverse',
             'tidygraph', 'ggraph', 'igraph','stringr','tidyr', 'ggplot2',
             'visNetwork', 'topicmodels', 'crosstalk', 'utf8')

for (p in packages) {
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

```

## 3.1 Data Preparation

First, I have loaded the news articles required for the first question. The code below will read individual text file as one id group and save it as a dataframe

```{r, echo = TRUE}

list_of_files <- "C:/Users/aaron/Documents/Aaron Documents/Semester 3/VA/Assignment/Raw Data/News Articles"

read_folder <- function (infolder) {
  tibble(file = dir(infolder,
                    full.names = TRUE)) %>%
    mutate(text = map(file,
                      read_lines)) %>%
    transmute(id=basename(file),
              text) %>%
    unnest(text)
}



raw_text <- tibble(folder =
                     dir(list_of_files,
                         full.names=TRUE)) %>%
  mutate(folder_out = map(folder,
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder),
            id, text)
write_rds(raw_text, "C:/Users/aaron/Documents/Aaron Documents/Semester 3/VA/Assignment/Raw Data/News Articles/allfiles.rds")

for (col in colnames(raw_text)){
  Encoding(raw_text[[col]]) <- "latin1"}

```

Next, I made use of a ggplot to ensure that the number of files uploaded matches the raw data. I have done a visual check with the files in my repository.

```{r, echo = FALSE}
raw_text %>% 
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y=NULL)
```

## 3.2 Data Cleaning

Next, I will carry out some data cleaning to make sure it can be evaluated. I have employed the following steps to clean the data. Where

1. Remove empty lines from the text column. 

```{r, echo = TRUE, eval=TRUE}
raw_text<- raw_text %>%
  #filter(raw_text$text != "") %>%
  filter(raw_text$text != " ")

head(raw_text, n=10)

```

2. Separate out the important variables: Location, Published Date, Source, Title and Main Text. This is done by using regex to identify the important keywords in the txt files. I have also renamed and tidied the data to allow data pivoting in the next step. 

```{r, echo = TRUE, eval=TRUE}
sep_text <- raw_text %>%
  separate(text, c('Type', 'entry'), "(?<=LOCATION):|(?<=PUBLISHED):|(?<=SOURCE):|(?<=TITLE):", remove=FALSE)

sep_text$entry[is.na(sep_text$entry)] <- sep_text$Type[is.na(sep_text$entry)] 
sep_text <- sep_text %>%
  mutate(Type = ifelse(entry==Type, "TEXT", Type))

sep_text <- sep_text %>%
  group_by(newsgroup,id,Type) %>%
  summarise (entry = paste(entry, collapse = " "))

sep_text <- sep_text %>%
  unite("newsgroup_id", newsgroup:id, sep = "_")

head(sep_text, n=10)

```

3. Using the pivot wider function to transform the important variables from rows to columns

```{r, echo = TRUE, eval=TRUE}

wide_text <- sep_text[,c("newsgroup_id","Type","entry")]
wide_text <- pivot_wider(wide_text,
                         names_from = Type,
                         values_from = entry)

wide_text_cleaned <- data.frame(lapply(wide_text,trimws))

head(wide_text_cleaned, n=10)

```


4. Ensuring that the data type is correct, specifically the published date. This can be done using the parse_date_time function

```{r, echo = TRUE, eval=TRUE}

wide_text_cleaned$PUBLISHED <- parse_date_time(wide_text_cleaned$PUBLISHED, orders=c("ymd", "dmy HM","mdy","dmy"))
wide_text_cleaned <- data.frame(wide_text_cleaned)

```

## 233 Data Preprocessing

5. In this step, I will categorise the data with trigrams, a function that splits the text column into consecutive words. I have also attempted to remove common stopwords from the trigrams. This variable will be used as the base for other derivations. 

```{r, echo = FALSE, eval=TRUE}

trigrams_news <- wide_text_cleaned %>%
  unnest_tokens(trigram, TEXT , token = "ngrams", n = 3)

trigrams_news <- trigrams_news %>%
  separate(trigram, into = c("first","second","third"), sep = " ", remove = FALSE) %>%
  anti_join(stop_words, by = c("first" = "word")) %>%
  anti_join(stop_words, by = c("second" = "word")) %>%
  anti_join(stop_words, by = c("third" = "word")) %>%
  filter(str_detect(first, "[a-z]") &
           str_detect(second, "[a-z]") &
           str_detect(third, "[a-z]"))
  
head(trigrams_news, n=10)

```

6. Next, I will append the count of each bigrams by newsgroup_id and words. This will show the number of unique bigrams in each news article. 

```{r, echo = FALSE, eval=TRUE}

words_by_newsgroup <- trigrams_news %>%
  count(newsgroup_id, trigram, sort=TRUE) %>%
  separate (newsgroup_id, c('newsgroup','id'), '_', remove=FALSE)

head(words_by_newsgroup, n=10)

```

7. In order to understand the relationship between different news articles, I will apply pairwise correlation on the bigrams, grouped by unique news articles.


```{r, echo = FALSE, eval=TRUE}
news_cors <- words_by_newsgroup %>%
  pairwise_cor(newsgroup_id,
               trigram,
               n,
               sort=TRUE)

head(news_cors,n=10)

```

8. As I will be plotting Network graph, there is a need to rename the columns to fit the requirements.

```{r, echo = FALSE, eval=TRUE}
news_cors_aggregated <- news_cors %>%
    rename(
    to = item1,
    from = item2)

news_cors_node <- wide_text_cleaned %>%
  rename(
    id = newsgroup_id,
    group = SOURCE
  )

```

9. Next, I noticed that there are duplicates in correlation from the earlier calculations.

```{r, echo = FALSE, eval=TRUE}
news_cors_aggregated_joined <- news_cors_aggregated %>%
  inner_join(news_cors_node, by = c("to"= "id")) %>%
  inner_join(news_cors_node, by = c("from" = "id"))

news_cors_aggregated_joined <- news_cors_aggregated_joined[c("from","to","correlation","PUBLISHED.x","PUBLISHED.y")]

news_cors_aggregated_joined_sort <- news_cors_aggregated_joined %>%
  arrange(desc(correlation), PUBLISHED.x)

head(news_cors_aggregated_joined_sort,n=10)

```

10. As such, we will only want to keep the correlation where the published date in the from column is earlier than the to column. This will ensure that the network graph plotted later will only show the primary sources pointing to the secondary sources. In the above step, I have arranged the data based on correlation and published date in the "from" column. The next step of code will remove every 2nd entry. 

```{r, echo = FALSE, eval=TRUE}

ind <- seq(1,nrow(news_cors_aggregated_joined_sort),by=2)
news_cors_aggregated_joined_sort_noduplicates <- news_cors_aggregated_joined_sort[ind,]

head(news_cors_aggregated_joined_sort_noduplicates,n=10)

```


11. I will do one final tidying of the data before loading into visNetwork. I have filtered out correlations that is less than 0.40 as we want to look at news that are more closely related. I have also kept the top-30 count of networks to streamline the data on the network plot.

```{r, echo = FALSE, eval=TRUE}
news_cors_aggregated_joined_sort_noduplicates_50 <- news_cors_aggregated_joined_sort_noduplicates %>%
  filter(correlation >= 0.40)


news_cors_aggregated_joined_sort_noduplicates_50_top_10 <- news_cors_aggregated_joined_sort_noduplicates_50 %>%
  count(from) %>%
  top_n(30)

news_cors_aggregated_joined_sort_noduplicates_final <- news_cors_aggregated_joined_sort_noduplicates_50 %>%
  inner_join(news_cors_aggregated_joined_sort_noduplicates_50_top_10, by = "from")


news_cors_node_filtered <- news_cors_node[(news_cors_node$id %in% news_cors_aggregated_joined_sort_noduplicates_final$from)| (news_cors_node$id %in% news_cors_aggregated_joined_sort_noduplicates_final$to),]

```

12. Plotting the visNetwork graph to understand the network.

```{r, echo = FALSE, eval=TRUE}
visNetwork(news_cors_node_filtered,
           news_cors_aggregated_joined_sort_noduplicates_final) %>%
  visEdges(arrows="to") %>%
  visIgraphLayout(layout = "layout_with_fr") %>%
  visOptions(highlightNearest = list(enabled = TRUE),
             nodesIdSelection = TRUE,
             selectedBy = "group") %>%
  visLayout(randomSeed = 123)


```

## 3.4 Analysis

1. From the network plot, we are able to differentiate the primary and the derivative sources. News articles that are closely correlated are linked together. The direction of the arrows will point from the primary source to the derivative source for articles that are closely related. 

![Figure 1: Classification of Primary and Derivative Source]()

2. From the above figure, "The Explainer_382" is the primary source for "The Abila Post_233" and "Central Bulletin_244" and subsequently used as a source for "News Online Today_489. 





# 5. Question 3 Analysis

## 5.1 Data Preparation

1. First, I have loaded the data provided - employees and email records. I have also done some simple data cleaning to remove information that are not useful for text analysis. I have also ensured that the data type is consistent.

```{r, echo = TRUE, eval=TRUE}
employees_records <- readxl::read_xlsx("C:/Users/aaron/Documents/Aaron Documents/Semester 3/VA/Assignment/Raw Data/EmployeeRecords.xlsx")
employees_records_cleaned <- employees_records %>%
  unite(fullname, FirstName, LastName, sep = " ", remove=FALSE) %>%
  mutate_if(is.character, utf8_encode)

employees_records_cleaned$fullname <- trimws(employees_records_cleaned$fullname, which = c("both"))

email_records <- read_csv("C:/Users/aaron/Documents/Aaron Documents/Semester 3/VA/Assignment/Raw Data/email headers.csv")
email_records_cleaned <- email_records %>%
  mutate(To = str_remove_all(To,"@gastech.com.kronos|@gastech.com.tethys")) %>%
  mutate(To = str_replace_all(To, "[.]", " ")) %>%
  mutate(From = str_remove_all(From,"@gastech.com.kronos|@gastech.com.tethys")) %>%
  mutate(From = str_replace_all(From, "[.]", " ")) %>%
  mutate(Date = parse_date_time(x = Date, orders =c("%m%d%y %H%M","%m%d%y"))) %>%
  mutate_if(is.character, utf8_encode)

email_records_cleaned2 <- strsplit(email_records_cleaned$To, split=",")
email_records_cleaned2 <- data.frame(From = rep(email_records_cleaned$From, sapply(email_records_cleaned2, length)), To = unlist(email_records_cleaned2), Date = rep(email_records_cleaned$Date, sapply(email_records_cleaned2, length)),Subject =  rep(email_records_cleaned$Subject, sapply(email_records_cleaned2, length)))
email_records_cleaned2$To <- trimws(email_records_cleaned2$To, which = c("both"))
email_records_cleaned2$From <- trimws(email_records_cleaned2$From, which = c("both"))
email_records_cleaned2$Subject <- gsub("[[:punct:]]", "", email_records_cleaned2$Subject)


```

2. Next, I will prepare the node and edge dataframe to build a network diagram using visNetowrk. This will allow us to view who are linked together. It indicates some form of communication and relationship between them. The node and edge dataframe needs to have pre-defined column name and format for the function to plot the relevant graph. The edge list is also aggregated to based on the sender, recipient and subject. 

```{r, echo = TRUE, eval=TRUE}

#Tidying node list
employees_records_cleaned_rename <- employees_records_cleaned %>%
  rename(id = fullname) %>%
  rename (group = CurrentEmploymentType) %>%
  arrange(id)

#Tidying edge list

email_records_cleaned2_rename <- email_records_cleaned2 %>%
  rename (from = From) %>%
  rename (to = To) %>%
  mutate(Subject = str_remove_all(Subject,"RE: "))

email_records_cleaned2_rename_aggregated <- email_records_cleaned2_rename %>%
  group_by (from, to, Subject) %>%
  summarise(Weight = n()) %>%
  filter(from!=to) %>%
  filter(to!=from) %>%
  filter(Weight > 1) %>%
  ungroup()

```

# 5.2 Data Preprocessing

1. We are now ready to plot a visNetwork graph to show communication between individual staff and departments

```{r, echo = TRUE, eval=TRUE}
visNetwork(employees_records_cleaned_rename,
           email_records_cleaned2_rename_aggregated) %>%
  visEdges(arrows="to") %>%
  visIgraphLayout(layout = "layout_with_fr") %>%
  visOptions(highlightNearest = TRUE,
             nodesIdSelection = TRUE,selectedBy = "group", collapse=TRUE) %>%
  visLayout(randomSeed = 123) %>%
  visLegend()
```

# 6. References

[1] UNSW Sydney. (n.d.). Primary and secondary sources. UNSW Sydney Library. https://www.library.unsw.edu.au/study/information-resources/primary-and-secondary-sources#:~:text=Primary%20sources%20provide%20a%20first,they%20can%20share%20new%20information.&amp;text=Secondary%20sources%20involve%20analysis%2C%20synthesis,or%20evaluation%20of%20primary%20sources. 




